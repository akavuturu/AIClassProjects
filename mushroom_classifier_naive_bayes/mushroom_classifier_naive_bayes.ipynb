{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "from math import log2\n",
    "from typing import List, Dict, Tuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Naive Bayes Classifier with the same data from last week:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "(You should have downloaded it).\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "You'll first need to calculate all of the necessary probabilities using a `train` function. A flag will control whether or not you use \"+1 Smoothing\" or not. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Tuples. Each Tuple has the best class in the first position and a dict with a key for every possible class label and the associated *normalized* probability. For example, if we have given the `classify` function a list of 2 observations, we would get the following back:\n",
    "\n",
    "```\n",
    "[(\"e\", {\"e\": 0.98, \"p\": 0.02}), (\"p\", {\"e\": 0.34, \"p\": 0.66})]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class label with the highest probability; you can write a simple function that takes the Dict and returns that class label.\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the *unnormalized* probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You will have the same basic functions as the last module's assignment and some of them can be reused or at least repurposed.\n",
    "\n",
    "`train` takes training_data and returns a Naive Bayes Classifier (NBC) as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, smoothing=True):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "The `smoothing` value defaults to True. You should handle both cases.\n",
    "\n",
    "`classify` takes a NBC produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data). (This is not the same `classify` as the pseudocode which classifies only one instance at a time; it can call it though).\n",
    "\n",
    "```\n",
    "def classify(nbc, observations, labeled=True):\n",
    "    # returns a list of tuples, the argmax and the raw data as per the pseudocode.\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application). If you did so last time, you can reuse it for this assignment.\n",
    "\n",
    "Following Module 3's discussion, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Naive Bayes Classifier algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. You will do this *twice*. Once with smoothing=True and once with smoothing=False. You should follow up with a brief explanation for the similarities or differences in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The data is given in \"agaricus-lepiota.data\" as a text file - each line in the text file is a single observation, with a total of 8124 observations, each with 22 attributes.\n",
    "\n",
    "| Index | Variable                  | Description |\n",
    "| ----- | -----------               | ----------- |\n",
    "| 0     | **class label**           | edible=e,poisonous=p |\n",
    "| 1     | cap-shape                 | bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s\n",
    "| 2     | cap-surface               | fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "| 3     | cap-color                 | brown=n,buff=b,cinnamon=c,gray=g,green=r,\n",
    "|       |                           | pink=p,purple=u,red=e,white=w,yellow=y\n",
    "| 4     | bruises?                  | bruises=t,no=f\n",
    "| 5     | odor                      | almond=a,anise=l,creosote=c,fishy=y,foul=f,\n",
    "|       |                           | musty=m,none=n,pungent=p,spicy=s\n",
    "| 6     | gill-attachment           | attached=a,descending=d,free=f,notched=n\n",
    "| 7     | gill-spacing              | close=c,crowded=w,distant=d\n",
    "| 8     | gill-size                 | broad=b,narrow=n\n",
    "| 9     | gill-color                | black=k,brown=n,buff=b,chocolate=h,gray=g,\n",
    "|       |                           | green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "| 10    | stalk-shape               | enlarging=e,tapering=t\n",
    "| 11    | stalk-root                | bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n",
    "| 12    | stalk-surface-above-ring  | fibrous=f,scaly=y,silky=k,smooth=s\n",
    "| 13    | stalk-surface-below-ring  | fibrous=f,scaly=y,silky=k,smooth=s\n",
    "| 14    | stalk-color-above-ring    | brown=n,buff=b,cinnamon=c,gray=g,orange=o,\n",
    "|       |                           | pink=p,red=e,white=w,yellow=y\n",
    "| 15    | stalk-color-below-ring    | brown=n,buff=b,cinnamon=c,gray=g,orange=o,\n",
    "|       |                           | pink=p,red=e,white=w,yellow=y\n",
    "| 16    | veil-type                 | partial=p,universal=u\n",
    "| 17    | veil-color                | brown=n,orange=o,white=w,yellow=y\n",
    "| 18    | ring-number               | none=n,one=o,two=t\n",
    "| 19    | ring-type                 | cobwebby=c,evanescent=e,flaring=f,large=l,\n",
    "|       |                           | none=n,pendant=p,sheathing=s,zone=z\n",
    "| 20    | spore-print-color         | black=k,brown=n,buff=b,chocolate=h,green=r,\n",
    "|       |                           | orange=o,purple=u,white=w,yellow=y\n",
    "| 21    | population                | abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n",
    "| 22    | habitat                   | grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n",
    "\n",
    "The 11th attribute has possible missing data, denoted by a \"?\" - any observations with missing data will not be included in the dataset. The class label is the first element in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three functions are taken from Programming Assignment 3 to parse data, create folds in data, and separate the data into training and test sets. The `parse_data` function is augmented to generate lists of characters rather than floats, and excludes rows with \"?\" characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"parse_data\"></a>\n",
    "## parse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [value for value in line.rstrip().split(\",\")]\n",
    "        if \"?\" not in datum: data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5644"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = parse_data(\"agaricus-lepiota.data\")\n",
    "len(data) # 8124 observations, 2480 observations with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'f', 's', 'g', 'f', 'n', 'f', 'w', 'b', 'k', 't', 'e', 'f', 's', 'w', 'w', 'p', 'w', 'o', 'e', 'k', 'a', 'g']\n"
     ]
    }
   ],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_folds\"></a>\n",
    "## create_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = create_folds(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_train_test\"></a>\n",
    "## create_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_train_test(folds, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5079"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train\n",
    "\n",
    "`train` takes data, a list of all class labels, and an optional smoothing parameter and computes probabilities for every attribute given a class label - $p(a_i | c_j)$. By counting the occurences of each attribute for a class label, and then dividing by the occurences of that class label, a probability is generated for each attribute/class-label pair. This is then used in the Naive Bayes Classifier to classify data probabilistically. The smoothing parameter determines whether the probabilities are calculated from `+ 1` smoothing, which initializes all instances of attribute/class label pairs to 1 instead of 0. This smoothing is done to minimize the impact of data where the attribute does not have an occurence for a class label (it \"smooths\" this data out and makes it closer to the other data points rather than at 0). The function returns a dictionary with keys as either class labels ($c$) or attribute/class label pairs ($attr\\_idx, attr\\_val, c$). **Used by**: [cross_validate](#cross_validate).\n",
    "\n",
    "* **training_data**: the data used to construct the probability dict from\n",
    "* **class_labels**: a list of all possible class labels\n",
    "* **smoothing**: optional parameter to implement `+ 1` smoothing\n",
    "\n",
    "**returns** `Dict`: a dict of probabilities for each attribute/class label pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, class_labels, smoothing=True):\n",
    "    possible_tuples, nbc, counts, start_val = [], {}, {}, 1 if smoothing else 0\n",
    "    for row in training_data:\n",
    "        counts[row[0]] = counts[row[0]] + 1 if row[0] in counts else 1 + start_val\n",
    "        for idx, attr in enumerate(row[1:]):\n",
    "            for label in class_labels:\n",
    "                if (idx, attr, label) not in possible_tuples: possible_tuples.append((idx, attr, label))\n",
    "    counts.update({tup:start_val for tup in possible_tuples})\n",
    "    for row in training_data:\n",
    "        for idx, attr in enumerate(row[1:]):\n",
    "            if (idx, attr, row[0]) in counts: \n",
    "                counts[(idx, attr, row[0])] += 1\n",
    "            else:\n",
    "                counts[(idx, attr, row[0])] = 1 + start_val\n",
    "    for k,v in counts.items():\n",
    "        nbc[k] = (v / counts[k[2]]) if type(k) == tuple else (v - start_val) / len(training_data)\n",
    "    return nbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 0.5333333333333333,\n",
       " 'y': 0.4666666666666667,\n",
       " (0, 'ro', 'y'): 0.625,\n",
       " (0, 'ro', 'n'): 0.4444444444444444,\n",
       " (1, 'la', 'y'): 0.875,\n",
       " (1, 'la', 'n'): 0.3333333333333333,\n",
       " (2, 'bl', 'y'): 0.125,\n",
       " (2, 'bl', 'n'): 0.4444444444444444,\n",
       " (0, 'sq', 'y'): 0.5,\n",
       " (0, 'sq', 'n'): 0.6666666666666666,\n",
       " (2, 'gr', 'y'): 0.625,\n",
       " (2, 'gr', 'n'): 0.3333333333333333,\n",
       " (1, 'sm', 'y'): 0.25,\n",
       " (1, 'sm', 'n'): 0.7777777777777778,\n",
       " (2, 'rd', 'y'): 0.5,\n",
       " (2, 'rd', 'n'): 0.4444444444444444}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assertions/unit tests\n",
    "data = [['n', 'ro', 'la', 'bl'], \n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['n', 'sq', 'sm', 'rd'],\n",
    "        ['y', 'ro', 'la', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'bl'],\n",
    "        ['n', 'ro', 'sm', 'bl'],\n",
    "        ['y', 'ro', 'sm', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'gr'],\n",
    "        ['y', 'ro', 'la', 'gr'],\n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['n', 'sq', 'la', 'rd'],\n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['y', 'ro', 'la', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'rd'],\n",
    "        ['n', 'ro', 'sm', 'gr']]\n",
    "nbc = train(data, ['y', 'n'], True)\n",
    "# assert nbc[\"n\"] == 8/15\n",
    "# assert nbc[(2, \"rd\", \"n\")] == 4/9\n",
    "\n",
    "\n",
    "# nbc = train(data, ['y', 'n'], False)\n",
    "# assert nbc[(2, \"rd\", \"n\")] == (3/8)\n",
    "nbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classify_one\n",
    "\n",
    "`classify_one` is a helper function that takes an observation, a dict of probabilities, and a list of class labels and classifies the observation. This function closely follows the pseudocode discussed in class. The function computes the probability of a given class label using the numerator of Bayes Rule:\n",
    "$$ P(C | A) \\propto P(A | C) * P(C) $$\n",
    "\n",
    "We can compute this for every class label $c$ as follows:\n",
    "\n",
    "$$ p(c | A) \\propto p(c) * \\prod_{i}^n p(a_i | c) $$\n",
    "\n",
    "The function creates a dictionary of these probabilities mapped to class labels as keys, and returns a tuple: `(majority_label, probs)` where `probs` is the above dictionary. The probabiliites are normalized by dividing by the sum of probabilities for each class label. **Used by**: [classify](#classify).\n",
    "\n",
    "* **observation**: a list of attributes representing one observation\n",
    "* **nbc**: a dict of probabilities for each attribute returned by `train`\n",
    "* **class_labels**: a list of all possible class labels\n",
    "\n",
    "**returns** `Tuple`: a tuple of the majority class label and a dict of all class labels mapped to their normalized probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_one(observation, nbc, class_labels):\n",
    "    probs = {}\n",
    "    for label in class_labels:\n",
    "        probs[label] = nbc[label]\n",
    "        for idx, attr in enumerate(observation):\n",
    "            probs[label] = probs[label] * nbc[(idx, attr, label)] if (idx, attr, label) in nbc else 0\n",
    "    sum_probs = sum(probs.values())\n",
    "    for k,v in probs.items():\n",
    "        probs[k] = v / sum_probs\n",
    "    majority_class = max(probs, key=probs.get)\n",
    "    probs = dict(sorted(probs.items(), key=lambda prob: prob[1], reverse=True))\n",
    "    return (majority_class, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assertions/unit tests\n",
    "data = [['n', 'ro', 'la', 'bl'], \n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['n', 'sq', 'sm', 'rd'],\n",
    "        ['y', 'ro', 'la', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'bl'],\n",
    "        ['n', 'ro', 'sm', 'bl'],\n",
    "        ['y', 'ro', 'sm', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'gr'],\n",
    "        ['y', 'ro', 'la', 'gr'],\n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['n', 'sq', 'la', 'rd'],\n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['y', 'ro', 'la', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'rd'],\n",
    "        ['n', 'ro', 'sm', 'gr']]\n",
    "class_labels = ['y', 'n']\n",
    "nbc = train(data, class_labels, True)\n",
    "classification = classify_one([\"sq\", \"la\", \"rd\"], nbc, class_labels)\n",
    "assert classification[1]['y'] - (0.102 / (0.102 + 0.053)) < 0.005\n",
    "assert classification[1]['n'] - (0.053 / (0.102 + 0.053)) < 0.005\n",
    "assert classification[0] == 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classify\n",
    "\n",
    "`classify` takes a list of observations and classifies each using the Naive Bayes Classifier algorithm implemented in `classify_one`. The function takes an optional parameter `labeled` that denotes whether data is labeled or unlabeled - if it is unlabeled, it appends an extra column to the front representing unknown class labels. The function returns a list of classifications for each observation in order. **Uses**: [classify_one](#classify_one). **Used by**: [cross_validate](#cross_validate).\n",
    "\n",
    "* **nbc**: a dict of probabilities for each attribute returned from `train`.\n",
    "* **observations**: a list of observations as lists of attributes\n",
    "* **class_labels**: a list of all possible class labels\n",
    "* **labeled**: an optional parameter denoting whether `observations` is labeled\n",
    "\n",
    "**returns** `List`: a list of classifications as tuples returned from `classify_one`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(nbc, observations, class_labels, labeled=True):\n",
    "    dataset = deepcopy(observations) if labeled else [[None] + deepcopy(row) for row in observations]\n",
    "    classifications = []\n",
    "    for row in dataset:\n",
    "        classification = classify_one(row[1:], nbc, class_labels)\n",
    "        classifications.append(classification)\n",
    "    return classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['n', 'ro', 'la', 'bl'], \n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['n', 'sq', 'sm', 'rd'],\n",
    "        ['y', 'ro', 'la', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'bl'],\n",
    "        ['n', 'ro', 'sm', 'bl'],\n",
    "        ['y', 'ro', 'sm', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'gr'],\n",
    "        ['y', 'ro', 'la', 'gr'],\n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['n', 'sq', 'la', 'rd'],\n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['y', 'ro', 'la', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'rd'],\n",
    "        ['n', 'ro', 'sm', 'gr']]\n",
    "class_labels = ['y', 'n']\n",
    "nbc = train(data, class_labels, True)\n",
    "obs = [[\"sq\", \"la\", \"rd\"]]\n",
    "classifications = classify(nbc, obs, class_labels, False)\n",
    "assert classifications[0][0] == \"y\"\n",
    "\n",
    "obs = [['n', 'ro', 'la', 'bl'],\n",
    "       ['n', 'sq', 'sm', 'bl'],\n",
    "       ['y', 'ro', 'la', 'gr']]\n",
    "\n",
    "classifications = classify(nbc, obs, class_labels, True)\n",
    "labels = [row[0][0] for row in classifications]\n",
    "assert labels == ['n', 'n', 'y']\n",
    "assert classifications[1][1]['n'] + classifications[1][1]['y'] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate\n",
    "\n",
    "`evaluate` takes a list of classifications and labeled data and returns the error rate of the classifications. Error rate is: \n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "**Used by**: [cross_validate](#cross_validate).\n",
    "\n",
    "* **labeled_data**: the real values for labels to compare to\n",
    "* **classifications**: the estimates to determine error rate for\n",
    "\n",
    "**returns** `float`: the error rate as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(labeled_data, classifications):\n",
    "    num_errors = 0\n",
    "    labels = [row[0] for row in labeled_data]\n",
    "    for actual_label, classified_label in zip(labels, classifications):\n",
    "        if actual_label != classified_label[0]: num_errors += 1\n",
    "    return num_errors / len(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assertions/unit tests\n",
    "data = [['n', 'ro', 'la', 'bl'], \n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['n', 'sq', 'sm', 'rd'],\n",
    "        ['y', 'ro', 'la', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'bl'],\n",
    "        ['n', 'ro', 'sm', 'bl'],\n",
    "        ['y', 'ro', 'sm', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'gr'],\n",
    "        ['y', 'ro', 'la', 'gr'],\n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['n', 'sq', 'la', 'rd'],\n",
    "        ['y', 'sq', 'la', 'gr'],\n",
    "        ['y', 'ro', 'la', 'rd'],\n",
    "        ['n', 'sq', 'sm', 'rd'],\n",
    "        ['n', 'ro', 'sm', 'gr']]\n",
    "class_labels = ['y', 'n']\n",
    "nbc = train(data, class_labels, True)\n",
    "classifications = classify(nbc, data, class_labels, True)\n",
    "assert evaluate(data, classifications) == 2/15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_validate\n",
    "\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation to `train`, `classify`, and `evaluate`. The function shuffles the data, splits the data into folds, and performs 10-fold cross validation on the folds. The error rate for each fold's evaluation is printed, and the average error rate is printed at the end. **Uses**: [create_folds](#create_folds), [create_train_test](#create_train_test), [train](#train), [classify](#classify), [evaluate](#evaluate).\n",
    "\n",
    "* **data**: the dataset\n",
    "* **smoothing**: a flag denoting whether to use `+ 1` smoothing in training\n",
    "* **class_labels**: a list of all possible class labels\n",
    "* **classify**: a function that takes a dict of probabilities, a set of observations, and a list of class labels and returns the classifications for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data, smoothing, class_labels, classify):\n",
    "    avg_err_rate = 0\n",
    "    random.shuffle(data)\n",
    "    folds = create_folds(data, 10)\n",
    "    print(\"Smoothing:\", smoothing)\n",
    "    for i in range(10):\n",
    "        train_data, test_data = create_train_test(folds, i)\n",
    "        nbc = train(train_data, class_labels, smoothing)\n",
    "        classifications = classify(nbc, test_data, class_labels)\n",
    "        error_rate = round(evaluate(test_data, classifications), 5)\n",
    "        print(\"Fold\", i, \"error rate:\", error_rate)\n",
    "        avg_err_rate += error_rate\n",
    "    avg_err_rate = round(avg_err_rate / 10, 5)\n",
    "    print(\"Avg. error rate:\", avg_err_rate)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing: True\n",
      "Fold 0 error rate: 0.02478\n",
      "Fold 1 error rate: 0.02655\n",
      "Fold 2 error rate: 0.03894\n",
      "Fold 3 error rate: 0.02655\n",
      "Fold 4 error rate: 0.0266\n",
      "Fold 5 error rate: 0.0195\n",
      "Fold 6 error rate: 0.01064\n",
      "Fold 7 error rate: 0.03546\n",
      "Fold 8 error rate: 0.03369\n",
      "Fold 9 error rate: 0.01418\n",
      "Avg. error rate: 0.02569\n",
      "\n",
      "Smoothing: False\n",
      "Fold 0 error rate: 0.00177\n",
      "Fold 1 error rate: 0.00177\n",
      "Fold 2 error rate: 0.00531\n",
      "Fold 3 error rate: 0.00177\n",
      "Fold 4 error rate: 0.00355\n",
      "Fold 5 error rate: 0.00177\n",
      "Fold 6 error rate: 0.00355\n",
      "Fold 7 error rate: 0.00177\n",
      "Fold 8 error rate: 0.00355\n",
      "Fold 9 error rate: 0.00355\n",
      "Avg. error rate: 0.00284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = parse_data(\"agaricus-lepiota.data\")\n",
    "cross_validate(data, True, ['e', 'p'], classify)\n",
    "cross_validate(data, False, ['e', 'p'], classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Here, we see an average error rate of about 0.03 for smoothed data and 0.003 for unsmoothed data. The smoothing is most likely hurting the classification by giving weight to incorrect associations - since the data is well-documented, there are lots of occurences of each attribute for all possible labels. Then, smoothing the data will be erroneous, since it only serves to decrease the effect of attribute pairs with few occurences. Since the data here comes from a field manual, it stands to reason that smoothing the data is meaningless - there is no statistical noise to smooth, and smoothing only introduces more error into our observations.\n",
    "\n",
    "In general, Naive Bayes performs well, considering it takes into account only joint conditional probabilities, and does not really \"learn\" in the same way a decision tree does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
