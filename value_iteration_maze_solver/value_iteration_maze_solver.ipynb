{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with Value Iteration\n",
    "\n",
    "These are the same maps from Module 1 but the \"physics\" of the world have changed. In Module 1, the world was deterministic. When the agent moved \"south\", it went \"south\". When it moved \"east\", it went \"east\". Now, the agent only succeeds in going where it wants to go *sometimes*. There is a probability distribution over the possible states so that when the agent moves \"south\", there is a small probability that it will go \"east\", \"north\", or \"west\" instead and have to move from there.\n",
    "\n",
    "There are a variety of ways to handle this problem. For example, if using A\\* search, if the agent finds itself off the solution, you can simply calculate a new solution from where the agent ended up. Although this sounds like a really bad idea, it has actually been shown to work really well in video games that use formal planning algorithms (which we will cover later). When these algorithms were first designed, this was unthinkable. Thank you, Moore's Law!\n",
    "\n",
    "Another approach is to use Reinforcement Learning which covers problems where there is some kind of general uncertainty in the actions. We're going to model that uncertainty a bit unrealistically here but it'll show you how the algorithm works.\n",
    "\n",
    "As far as RL is concerned, there are a variety of options there: model-based and model-free, Value Iteration, Q-Learning and SARSA. You are going to use Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The World Representation\n",
    "\n",
    "As before, we're going to simplify the problem by working in a grid world. The symbols that form the grid have a special meaning as they specify the type of the terrain and the cost to enter a grid cell with that type of terrain:\n",
    "\n",
    "```\n",
    "token   terrain    cost \n",
    ".       plains     1\n",
    "*       forest     3\n",
    "^       hills      5\n",
    "~       swamp      7\n",
    "x       mountains  impassible\n",
    "```\n",
    "\n",
    "When you go from a plains node to a forest node it costs 3. When you go from a forest node to a plains node, it costs 1. You can think of the grid as a big graph. Each grid cell (terrain symbol) is a node and there are edges to the north, south, east and west (except at the edges).\n",
    "\n",
    "There are quite a few differences between A\\* Search and Reinforcement Learning but one of the most salient is that A\\* Search returns a plan of N steps that gets us from A to Z, for example, A->C->E->G.... Reinforcement Learning, on the other hand, returns  a *policy* that tells us the best thing to do in **every state.**\n",
    "\n",
    "For example, the policy might say that the best thing to do in A is go to C. However, we might find ourselves in D instead. But the policy covers this possibility, it might say, D->E. Trying this action might land us in C and the policy will say, C->E, etc. At least with offline learning, everything will be learned in advance (in online learning, you can only learn by doing and so you may act according to a known but suboptimal policy).\n",
    "\n",
    "Nevertheless, if you were asked for a \"best case\" plan from (0, 0) to (n-1, n-1), you could (and will) be able to read it off the policy because there is a best action for every state. You will be asked to provide this in your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same costs as before. Note that we've negated them this time because RL requires negative costs and positive rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = { '.': -1, '*': -3, '^': -5, '~': -7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a list of offsets for `cardinal_moves`. You'll need to work this into your **actions**, A, parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinal_moves = [(0,-1), (1,0), (0,1), (-1,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinal_actions = {\"up\": (0,-1), \"right\": (1,0), \"down\": (0,1), \"left\": (-1,0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Value Iteration, we require knowledge of the *transition* function, as a probability distribution.\n",
    "\n",
    "The transition function, T, for this problem is 0.70 for the desired direction, and 0.10 each for the other possible directions. That is, if the agent selects \"north\" then 70% of the time, it will go \"north\" but 10% of the time it will go \"east\", 10% of the time it will go \"west\", and 10% of the time it will go \"south\". If agent is at the edge of the map, it simply bounces back to the current state.\n",
    "\n",
    "You need to implement `value_iteration()` with the following parameters:\n",
    "\n",
    "+ world: a `List` of `List`s of terrain (this is S from S, A, T, gamma, R)\n",
    "+ costs: a `Dict` of costs by terrain (this is part of R)\n",
    "+ goal: A `Tuple` of (x, y) stating the goal state.\n",
    "+ reward: The reward for achieving the goal state.\n",
    "+ actions: a `List` of possible actions, A, as offsets.\n",
    "+ gamma: the discount rate\n",
    "\n",
    "you will return a policy: \n",
    "\n",
    "`{(x1, y1): action1, (x2, y2): action2, ...}`\n",
    "\n",
    "Remember...a policy is what to do in any state for all the states. Notice how this is different than A\\* search which only returns actions to take from the start to the goal. This also explains why reinforcement learning doesn't take a `start` state.\n",
    "\n",
    "You should also define a function `pretty_print_policy( cols, rows, policy)` that takes a policy and prints it out as a grid using \"^\" for up, \"<\" for left, \"v\" for down and \">\" for right. Use \"x\" for any mountain or other impassable square. Note that it doesn't need the `world` because the policy has a move for every state. However, you do need to know how big the grid is so you can pull the values out of the `Dict` that is returned.\n",
    "\n",
    "```\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    ">>>>>>v\n",
    "^^^>>>v\n",
    "^^^>>>v\n",
    "^^^>>>G\n",
    "```\n",
    "\n",
    "(Note that that policy is completely made up and only illustrative of the desired output). Please print it out exactly as requested: **NO EXTRA SPACES OR LINES**.\n",
    "\n",
    "* If everything is otherwise the same, do you think that the path from (0,0) to the goal would be the same for both A\\* Search and Q-Learning?\n",
    "* What do you think if you have a map that looks like:\n",
    "\n",
    "```\n",
    "><>>^\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>G\n",
    "```\n",
    "\n",
    "has this converged? Is this a \"correct\" policy? What are the problems with this policy as it is?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_world(filename):\n",
    "    result = []\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 0:\n",
    "                result.append(list(line.strip()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init_rewards\n",
    "\n",
    "`init_rewards` initializes the `R` matrix for value iteration. It takes a reward for the goal, a goal coordinate, a list of costs for cells in the world, and a weight to apply to the costs. The function returns a fully initialized matrix that represents the rewards of each cell in the world. It assigns the goal cell a value of `reward`, mountains/impassable terrain a value of `None`, and all other cells a value equal to the cost of the cell multiplied by the `weight` parameter. In `value_iteration`, this weight is set to 5. **Used by**: [value_iteration](#value_iteration).\n",
    "\n",
    "* **reward**: the reward for the goal\n",
    "* **goal**: the goal coordinates\n",
    "* **costs**: a dict of costs for each type of cell in `world`\n",
    "* **world**: the world matrix\n",
    "* **weight**: a weight to multiply `costs`\n",
    "\n",
    "**returns** `List[List]]`: an initialized `R` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rewards(reward, goal, costs, world, weight):\n",
    "    rows, cols = len(world), len(world[0])\n",
    "    R = [[0 for x in range(cols)] for y in range(rows)]\n",
    "    for y in range(rows):\n",
    "        for x in range(cols):\n",
    "            if (x, y) == goal:\n",
    "                R[y][x] = reward\n",
    "            elif world[y][x] in costs.keys():\n",
    "                R[y][x] = costs[world[y][x]] * weight\n",
    "            else:\n",
    "                R[y][x] = None\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assertions/unit tests\n",
    "test = [['.', '.', '.', '.', '.', '.'], \n",
    "        ['.', '*', '*', '*', '*', '.'], \n",
    "        ['.', '*', '*', '*', '*', '.'], \n",
    "        ['.', '*', '*', 'x', '*', '.'], \n",
    "        ['.', '*', '*', '*', '*', '.'], \n",
    "        ['.', '.', '.', '.', '.', '.'], \n",
    "        ['.', '.', '.', '.', '.', '.']]\n",
    "costs = { '.': -1, '*': -3, '^': -5, '~': -7}\n",
    "reward = 100\n",
    "goal = (5, 6)\n",
    "R = init_rewards(reward, goal, costs, test, 1)\n",
    "assert R[0][0] == -1\n",
    "assert R[6][5] == 100\n",
    "assert R[3][3] == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bellman\n",
    "\n",
    "`bellman` applies the Bellman equation to `state` using the `R` and `V_last` matrices, as well as a list of actions and a discount rate `gamma`. The equation is:\n",
    "$$ Q[s, a] = R[s, a] + \\gamma * \\sum_{s'} T[s, a, s'] * V_{last}[s'] $$\n",
    "\n",
    "In this implementation, the reward is found from `R` and the transition model has a probability of 70% for the intended `action`, with an equal distribution for all other legal actions (ones that do not involve traveling out of bounds/reaching impassable terrain). The possible actions are instantiated and the resulting `Q` value is returned if possible - otherwise, `None` is returned to signify that the current state is impassable. **Used by**: [value_iteration](#value_iteration)\n",
    "\n",
    "* **state**: a tuple of (x, y, action)\n",
    "* **R**: the reward matrix\n",
    "* **V**: the value matrix\n",
    "* **actions**: a list of all possible actions\n",
    "* **gamma**: the discount rate\n",
    "* **rows**: the number of rows in `world`\n",
    "* **cols**: the number of columns in `world`\n",
    "\n",
    "**returns** `Float` or `None`: a value for the quality of the action or `None` for impassable terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman(state, R, V_last, actions, gamma, rows, cols):\n",
    "    (x, y, action), possible_actions = state, []\n",
    "    r_sa = R[y][x]\n",
    "    if r_sa:\n",
    "        (x_p, y_p) = (x + action[0], y + action[1])\n",
    "        if 0 <= x_p < cols and 0 <= y_p < rows and V_last[y_p][x_p] != None:\n",
    "            res = r_sa + gamma*V_last[y_p][x_p]*0.7\n",
    "            for surprise_action in actions:\n",
    "                (x_p, y_p) = (x + surprise_action[0], y + surprise_action[1])\n",
    "                if surprise_action != action and 0 <= x_p < cols and 0 <= y_p < rows and V_last[y_p][x_p] != None:\n",
    "                    possible_actions.append((surprise_action, x_p, y_p))\n",
    "            for (possible_action, x_p, y_p) in possible_actions:\n",
    "                res += gamma*V_last[y_p][x_p]*(0.3 / len(possible_actions))\n",
    "            return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assertions/unit tests\n",
    "test = [['.', '.', '.', '.', '.', '.'], \n",
    "        ['.', '*', '*', '*', '*', '.'], \n",
    "        ['.', '*', '*', '*', '*', '.'], \n",
    "        ['.', '*', '*', 'x', '*', '.'], \n",
    "        ['.', '*', '*', '*', '*', '.'], \n",
    "        ['.', '.', '.', '.', '.', '.'], \n",
    "        ['.', '.', '.', '.', '.', '.']]\n",
    "V_last = [[5, 5, 5, 5, 5, 5], \n",
    "          [4, 4, 4, 4, 4, 4],\n",
    "          [3, 3, 3, 3, 3, 3], \n",
    "          [2, 2, 2, 2, 2, 2], \n",
    "          [1, 1, 1, 1, 1, 1], \n",
    "          [-1, -1, -1, -1, -1, -1], \n",
    "          [0, 0, 0, 0, 0, 0]]\n",
    "R = [[-1, -1, -1, -1, -1, -1], \n",
    "     [-1, -3, -3, -3, -3, -1], \n",
    "     [-1, -3, -3, -3, -3, -1], \n",
    "     [-1, -3, -3, None, -3, -1], \n",
    "     [-1, -3, -3, -3, -3, -1], \n",
    "     [-1, -1, -1, -1, -1, -1], \n",
    "     [-1, -1, -1, -1, -1, -1]]\n",
    "q = bellman((0, 0, (0, 1)), R, V_last, cardinal_moves, 0.9, len(test), len(test[0]))\n",
    "assert q == 2.87\n",
    "\n",
    "q = bellman((0, 0, (0, -1)), R, V_last, cardinal_moves, 0.9, len(test), len(test[0]))\n",
    "assert q == None\n",
    "\n",
    "q = bellman((3, 5, (0, 1)), R, V_last, cardinal_moves, 0.9, len(test), len(test[0]))\n",
    "assert q == -1.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argmax\n",
    "\n",
    "`argmax` takes a `Q` matrix and a set of x-y coordinates, as well as a list of actions, and returns the action that corresponds to the best value in `Q` for that action from the given x-y coordinates. If no such action exists, the function returns (0, 0) to signify that the current state is impassable terrain. **Used by**: [value_iteration](#value_iteration).\n",
    "\n",
    "* **Q**: the Q dictionary of states and actions\n",
    "* **x**: the x coordinate of the current state\n",
    "* **y**: the y coordinate of the current state\n",
    "* **actions**: a list of all possible actions\n",
    "\n",
    "**returns** `Tuple`: a tuple of the best move to take, or (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(Q, x, y, actions):\n",
    "    max_reward, policy = -1000, (0, 0)\n",
    "    for action in actions:\n",
    "        if (x, y, action) in Q: \n",
    "            curr_reward = deepcopy(Q[(x, y, action)])\n",
    "            if curr_reward and curr_reward > max_reward:\n",
    "                max_reward = curr_reward\n",
    "                policy = deepcopy(action)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assertions/unit tests\n",
    "Q = {(0, 0, (0, 1)): 10, (0, 0, (1, 0)): 5}\n",
    "policy = argmax(Q, 0, 0, cardinal_moves)\n",
    "assert policy == (0, 1)\n",
    "\n",
    "Q = {(5, 5, (0, 1)): 5, (5, 5, (1, 0)): 5, (5, 5, (-1, 0)): 5, (5, 5, (0, -1)): 5}\n",
    "policy = argmax(Q, 5, 5, cardinal_moves)\n",
    "assert policy == (0, -1)\n",
    "\n",
    "Q = {(5, 5, (0, 1)): 5, (5, 5, (1, 0)): 5, (5, 5, (-1, 0)): 5, (5, 5, (0, -1)): 5}\n",
    "policy = argmax(Q, 0, 0, cardinal_moves)\n",
    "assert policy == (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute_err\n",
    "\n",
    "`compute_err` finds the max element-wise difference between `V` and `V_last` to determine when to stop iterating in `value_iteration`. The function compares all non-`None` cells and finds the max difference between the two to return as the max error. **Used by**: [value_iteration](#value_iteration).\n",
    "\n",
    "* **V**: the current value matrix\n",
    "* **V_last**: the previous value matrix\n",
    "\n",
    "**returns** `Float`: a float of the max element-wise differnce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_err(V, V_last):\n",
    "    max_err = 0\n",
    "    for y in range(len(V)):\n",
    "        for x in range(len(V[y])):\n",
    "            if V[y][x] != None and V_last[y][x] != None: \n",
    "                max_err = max(max_err, abs(V[y][x] - V_last[y][x]))\n",
    "    return max_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assertions/unit tests\n",
    "m1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "m2 = [[1, 2, 3], [6, 5, 4], [7, 8, 9]]\n",
    "err = compute_err(m1, m2)\n",
    "assert err == 2\n",
    "\n",
    "m1 = [[100, 101, 102], [103, 104, 105], [106, 107, 108]]\n",
    "m2 = [[100, 0, 102], [103, 104, 105], [101, 102, 103]]\n",
    "err = compute_err(m1, m2)\n",
    "assert err == 101\n",
    "\n",
    "m1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "m2 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "err = compute_err(m1, m2)\n",
    "assert err == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## value_iteration\n",
    "\n",
    "`value_iteration` implements the Vaue Iteration algorithm for solving reinforcement learning problems using the Bellman equation (see [bellman](#bellman)). The algorithm starts with a world map, a list of costs, a goal state, a reward for reaching the goal, a list of actions, and a discount rate, and creates a policy which is iteratively improved.\n",
    "\n",
    "The algorithm utilizes four different structures: the policy dictionary (`Pi`), which keeps track of the current iteration's best move for each cell; the reward matrix (`R`), which maps each cell in the world to a static payoff/reward using a weight of 5; the value matrix (`V` and `V_last`), which keeps track of the value/utility of traversing each cell in the matrix; and the Q dictionary (`Q`), which maps a state and action tuple (`x`, `y`, `action`) to a possible payoff. In each iteration, the algorithm decides which action is best for each state by considering the previous iteration's `V` matrix, and computing rewards for potential successors using the Bellman equation. The algorithm iterates until the maximum element-wise difference between two iterations' value matrices is less than a desired $\\epsilon$ value, which is set at $1*10^{-16}$. The function returns the policy matrix, which enumerates the best move for each cell in `world` to reach `goal`.\n",
    "\n",
    "**Uses**: [init_rewards](#init_rewards), [bellman](#bellman), [argmax](#argmax), [compute_err](#compute_err)\n",
    "\n",
    "* **world**: the world map as a list of lists\n",
    "* **costs**: the costs dictionary for each cell type in `world`\n",
    "* **goal**: the goal coordinates as a tuple (x, y)\n",
    "* **reward**: the reward for reaching the goal\n",
    "* **gamma**: the discount rate used in the Bellman Equation\n",
    "\n",
    "**returns**: `List[List]]`: a policy dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(world, costs, goal, reward, actions, gamma):\n",
    "    t, err, epsilon, limit, rows, cols = 0, 1, 1e-16, 3000, len(world), len(world[0])\n",
    "    V = [[0 for y in range(cols)] for x in range(rows)]\n",
    "    Q, Pi, R = {}, {}, init_rewards(reward, goal, costs, world, 5)\n",
    "    while err > epsilon:\n",
    "        t += 1\n",
    "        V_last = deepcopy(V)\n",
    "        for y, row in enumerate(world):\n",
    "            for x, cell in enumerate(row):\n",
    "                for action in actions:\n",
    "                    Q[(x, y, action)] = bellman((x, y, action), R, V_last, actions, gamma, rows, cols)\n",
    "                Pi[(x, y)] = argmax(Q, x, y, actions)\n",
    "                V[y][x] = Q[(x, y, Pi[(x, y)])] if Pi[(x, y)] != (0, 0) else None\n",
    "        err = compute_err(V, V_last)\n",
    "    return Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretty_print_policy\n",
    "\n",
    "`pretty_print_policy` takes a policy dictionary, dimensions for the `world`, and a `goal` state, and prints a \"prettier\" version of the policy. For each coordinate pair in `world`, the function looks up the best move to take from the `policy` and represents it in ASCII. The function also replaces mountains with \"x\" and the goal state with \"G\".\n",
    "\n",
    "**returns** `None`: the function prints the pretty policy as a side-effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_policy(cols, rows, policy, goal):\n",
    "    pretty_policy = [[\"0\" for x in range(cols)] for y in range(rows)]\n",
    "    move_lookup = {(0, 1): \"v\", (1, 0): \">\", (0, -1): \"^\", (-1, 0): \"<\"}\n",
    "    for y in range(rows):\n",
    "        for x in range(cols):\n",
    "            pretty_policy[y][x] = \"x\" if (policy[(x, y)] == (0, 0)) else move_lookup[policy[(x, y)]]\n",
    "    pretty_policy[goal[1]][goal[0]] = \"G\"\n",
    "    for row in pretty_policy:\n",
    "        print(\"\".join(row))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "### Small World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_world = read_world( \"small.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = (len(small_world[0])-1, len(small_world)-1)\n",
    "gamma = 0.9\n",
    "reward = 150\n",
    "\n",
    "small_policy = value_iteration(small_world, costs, goal, reward, cardinal_moves, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v>>>>v\n",
      "vvv>vv\n",
      "vvv>vv\n",
      "vvvxvv\n",
      "vvvvvv\n",
      ">>>>vv\n",
      ">>>>>G\n"
     ]
    }
   ],
   "source": [
    "cols = len(small_world[0])\n",
    "rows = len(small_world)\n",
    "\n",
    "pretty_print_policy(cols, rows, small_policy, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_world = read_world( \"large.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goal = (len(large_world[0])-1, len(large_world)-1) # Lower Right Corner FILL ME IN\n",
    "gamma = 0.9\n",
    "reward = 100000\n",
    "\n",
    "large_policy = value_iteration(large_world, costs, goal, reward, cardinal_moves, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v>>>v>>>>>>>>>>vv<>>>>>>>vv\n",
      "v>>>>>>>>>>>>>vvv<xxxxxxxvv\n",
      "vv^^xx>>>>>>>>>vvxxxvv<xxvv\n",
      "vvv^<xxx>>>>>>>>>>>vvv<xxvv\n",
      "v<v<<xxvv>>>>>>>>>>vvvxxxvv\n",
      "vvv<xxvvv>>>>>>>>>>>vvvxvvv\n",
      "vvvxx>vvvv^^xxx>>>>>>v>vvvv\n",
      "vvv>>>vvvv<^<<xxx>>>>>vvvvv\n",
      "v>>>>>vvv<<<<<xx>>>>>>>vvvv\n",
      ">>>>>>vvv<<xxxx>>>>>>>>vvvv\n",
      ">>>>>vvvv<xxx>>>>>vvxxxvvvv\n",
      ">>>>>vvvvxxv>>>>>>vvvxxvvvv\n",
      ">>>>>>vvvxxv>>>>>>>vvx>vvvv\n",
      ">>>>>>v>>vvv>>>>>>>>>>>vvvv\n",
      ">>>^x>v>>vvv<>>>>>>>>^xvvvv\n",
      "vv^xxx>>>vvvxxx>>>>^^xxvvvv\n",
      "vvxx>>>>>>>>vvxxx^^xxxvvvvv\n",
      "vvvxx>>>>>>>>>vvxxxx>>vvvvv\n",
      "vvvxxx>>>>>>>>>>>>>>>>vvvvv\n",
      "vvvvxxx>>>>>>>>>>>>>>>vvvvv\n",
      "vvvvvvxx>>>>^^x>>>>>>>vvvvv\n",
      ">>>vvvvxxx^^xx>>>>>>>>vvvvv\n",
      ">>>>>>>vvxxxx>>>>>>>>>>vvvv\n",
      ">>>>>>>>>>>>>>>^^xx>>>>vvvv\n",
      "^x>>>>>>>vvxxx^^xxvxx>>vvvv\n",
      "vxxx>>>>>>>vxxxx>>>vxxx>>vv\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>G\n"
     ]
    }
   ],
   "source": [
    "cols = len(large_world[0])\n",
    "rows = len(large_world)\n",
    "\n",
    "pretty_print_policy( cols, rows, large_policy, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_1 = [\n",
    "    ['.', '.', '.', '.'],\n",
    "    ['.', '~', '.', '.'],\n",
    "    ['*', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.']\n",
    "]\n",
    "\n",
    "goal = (len(world_1) - 1, len(world_1) - 1)\n",
    "gamma = 0.9\n",
    "reward = 500\n",
    "policy = value_iteration(world_1, costs, goal, reward, cardinal_moves, gamma)\n",
    "\n",
    "cols = len(world_1)\n",
    "rows = len(world_1)\n",
    "pretty_print_policy(cols, rows, policy, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
